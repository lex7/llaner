{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEX\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALEX\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': 0.85501224, 'word': 'ГК РФ', 'start': 53, 'end': 58}, {'entity_group': 'ORG', 'score': 0.731745, 'word': 'Правительства РФ', 'start': 93, 'end': 109}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\", do_lower_case=False)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "example = \"Настоящая документация подготовлена в соответствии с ГК РФ, Законом № 223-ФЗ, Постановлением Правительства РФ № 925.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\n",
    "\n",
    "    \"Документация о закупке (документация) – настоящая документация, содержащая установленные Законом № 223-ФЗ и Положением о закупках сведения о запросе предложений.\",\n",
    "\n",
    "    \"Извещение о закупке – документ, формируемый посредством функционала ЕИС и содержащий установленные Законом № 223-ФЗ и Положением о закупках сведения о запросе предложений.\",\n",
    "\n",
    "    \"Регламент работы ЭТП – документы оператора ЭТП, регламентирующие порядок проведения закупок на ЭТП в соответствии с Законом № 223-ФЗ, определяющие деятельность оператора ЭТП по обеспечению проведения закупок в соответствии с Законом № 223-ФЗ. \",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,  14418,  11191,  57772,  12942,    555,  10234,  86613,  11557,\n",
      "            113,  85065,  38878,    114,    100,  32001,  10752,  10385,  26325,\n",
      "          85065,  38878,    117,  10956,  22605,  17254,  26325,  68855,  11194,\n",
      "          94493,   1779,  24415,    118,  95689,    549,  45784,  63935,  28788,\n",
      "            555,  10234,  86613,  20265,  71088,    555,  10234,  99842,  10205,\n",
      "          23807,  99187,  11550,    119,    102,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101,  19597,  14149,  26692,    555,  10234,  86613,  11557,    100,\n",
      "          85065,    117,  40391,  14360,  69167,  87953,    561,  16657,  10510,\n",
      "          86228,  52486,    514,  27735,  14791,    549,  10956,  22605,  17254,\n",
      "          27312,  68855,  11194,  94493,   1779,  24415,    118,  95689,    549,\n",
      "          45784,  63935,  28788,    555,  10234,  86613,  20265,  71088,    555,\n",
      "          10234,  99842,  10205,  23807,  99187,  11550,    119,    102,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [   101, 101403,  83379,  28252,  18447,    538,  20411,  16027,    100,\n",
      "         108397,  95046,  10179,    538,  20411,  16027,    117,    557,  55577,\n",
      "          83379,  28252,  23312,  14611,  25778,  74545,  57728,  10234,  86613,\n",
      "          11899,  10122,    538,  20411,  16027,    543,  43451,    558,  94493,\n",
      "           1779,  24415,    118,  95689,    117,    555,  35415,  87097,  13826,\n",
      "          23249,  34112,  95046,  10179,    538,  20411,  16027,  10297,  70135,\n",
      "          10513,  19820,  89039,  57728,  10234,  86613,  11899,    543,  43451,\n",
      "            558,  94493,   1779,  24415,    118,  95689,    119,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'До', '##ку', '##мента', '##ция', 'о', 'за', '##куп', '##ке', '(', 'документ', '##ация', ')', '[UNK]', 'нас', '##то', '##я', '##щая', 'документ', '##ация', ',', 'со', '##дер', '##жа', '##щая', 'установлен', '##ные', 'Законом', '№', '223', '-', 'ФЗ', 'и', 'Пол', '##ожен', '##ием', 'о', 'за', '##куп', '##ках', 'сведения', 'о', 'за', '##прос', '##е', 'пред', '##ложен', '##ий', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Из', '##ве', '##щение', 'о', 'за', '##куп', '##ке', '[UNK]', 'документ', ',', 'форми', '##ру', '##емый', 'посредством', 'ф', '##ун', '##к', '##цио', '##нала', 'Е', '##И', '##С', 'и', 'со', '##дер', '##жа', '##щий', 'установлен', '##ные', 'Законом', '№', '223', '-', 'ФЗ', 'и', 'Пол', '##ожен', '##ием', 'о', 'за', '##куп', '##ках', 'сведения', 'о', 'за', '##прос', '##е', 'пред', '##ложен', '##ий', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Рег', '##лам', '##ент', 'работы', 'Э', '##Т', '##П', '[UNK]', 'документы', 'оператор', '##а', 'Э', '##Т', '##П', ',', 'р', '##ег', '##лам', '##ент', '##ир', '##ую', '##щие', 'порядок', 'проведения', 'за', '##куп', '##ок', 'на', 'Э', '##Т', '##П', 'в', 'соответствии', 'с', 'Законом', '№', '223', '-', 'ФЗ', ',', 'о', '##пр', '##еде', '##ля', '##ющие', 'деятельность', 'оператор', '##а', 'Э', '##Т', '##П', 'по', 'обе', '##с', '##пе', '##чению', 'проведения', 'за', '##куп', '##ок', 'в', 'соответствии', 'с', 'Законом', '№', '223', '-', 'ФЗ', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = encoded_input[\"input_ids\"]\n",
    "\n",
    "converted_sentences = [tokenizer.convert_ids_to_tokens(ids) for ids in tokenized_sentences]\n",
    "\n",
    "for tokens in converted_sentences:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Документация', 'о', 'закупке', '(', 'документация', ')', '[UNK]', 'настоящая', 'документация', ',', 'содержащая', 'установленные', 'Законом', '№', '223', '-', 'ФЗ', 'и', 'Положением', 'о', 'закупках', 'сведения', 'о', 'запросе', 'предложений', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Извещение', 'о', 'закупке', '[UNK]', 'документ', ',', 'формируемый', 'посредством', 'функционала', 'ЕИС', 'и', 'содержащий', 'установленные', 'Законом', '№', '223', '-', 'ФЗ', 'и', 'Положением', 'о', 'закупках', 'сведения', 'о', 'запросе', 'предложений', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'Регламент', 'работы', 'ЭТП', '[UNK]', 'документы', 'оператора', 'ЭТП', ',', 'регламентирующие', 'порядок', 'проведения', 'закупок', 'на', 'ЭТП', 'в', 'соответствии', 'с', 'Законом', '№', '223', '-', 'ФЗ', ',', 'определяющие', 'деятельность', 'оператора', 'ЭТП', 'по', 'обеспечению', 'проведения', 'закупок', 'в', 'соответствии', 'с', 'Законом', '№', '223', '-', 'ФЗ', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store reconstructed full sentences\n",
    "reconstructed_sentences = []\n",
    "\n",
    "# Iterate through the tokenized sentences\n",
    "for tokens in tokenized_sentences:\n",
    "    current_sentence = []  # Create an empty list for the current sentence\n",
    "    for token_id in tokens:\n",
    "        token = tokenizer.convert_ids_to_tokens([token_id.item()])[0]\n",
    "        if token.startswith(\"##\"):  # Check if it's a subword token\n",
    "            current_sentence[-1] += token[2:]  # Append to the last token in the current sentence\n",
    "        else:\n",
    "            current_sentence.append(token)  # Start a new token for the current sentence\n",
    "    reconstructed_sentences.append(current_sentence)  # Add the current sentence to the list\n",
    "\n",
    "# Print the reconstructed sentences\n",
    "for sentence in reconstructed_sentences:\n",
    "    print(sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
